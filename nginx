global CLI
> nginx -s reload
> nginx -t -c /etc/nginx/nginx.conf

** if there are many location directives, it will stop at the first match


server {
        listen 80;
        server_name 127.0.0.1 localhost;
        root /etc/nginx/html;

        location /app1/ {
                index index.html;
        }

        location  /app2/ {
                index home.html;
        }
#        location ~ \.(png|jpg|jpeg)$ {
#                expires max;
#        }

#        location ~* \.(png|jpg|jpeg)$ {
#                expires 10d;
#        }
        location ~* \.(jpg|jpeg)$ {
                root /etc/nginx/html/common;
        }
        location ~* \.(png|jpg|jpeg)$ {
                root /etc/nginx/html/common;
                try_files $uri $uri/ /nginx.png =404;
        }
}

location ∼* \.(png)$ {
        root /etc/nginx/html/common;
        try_files $uri $uri/ /nginx.png @mylocation;
}

location @mylocation{
        ...
        #do something here
        ...
}

Change directory (common to vendor_assets)
server {
        listen 80;
        server_name 127.0.0.1 localhost;
        root /etc/nginx/html;

        location /common/ {
                rewrite ^(/common/)(.*) /vendor_assets/$2 last;
        }
}

Error handle
It can be used inside http, server, location, and if blocks.
error_page   500 502 503 504  /50x.html;
    location = /50x.html {
        root   /usr/share/nginx/html;
    }

DENY ACCESS TO ANY SPECIFIC LOCATION
server {
        listen 80;
        server_name 127.0.0.1 localhost;
        root /etc/nginx/html;

        location /vendor_assets/ {
                deny all;
        }
}

PROXY THE REQUESTS TO APACHE
location ∼ \.php$ {
        proxy_pass   http://127.0.0.1;
    }
    
PROXY THE REQUESTS TO FASTCGI
location ∼ \.php$ {
        root           html;
        fastcgi_pass   127.0.0.1:9000;
        fastcgi_index  index.php;
        fastcgi_param  SCRIPT_FILENAME  /scripts$fastcgi_script_name;
        include        fastcgi_params;
    }
	
# INTERNAL REDIRECTS
server {
    listen      80;
    server_name www.site1.com;
    return      301 http://site1.com$request_uri;
}
server {
    listen       80;
    server_name  site1.com;

    root  "/usr/share/nginx/html/site1/Shield Theme";

    location / {
        index  index.html index.htm;
    }

    error_page   500 502 503 504  /50x.html;
    location = /50x.html {
        root   /usr/share/nginx/html;
    }
}

# SITES USING DIFFERENT PORTS
- you can configure server blocks in a way that multiple server blocks with the same name exist, 
but are listening on different ports.
server {
	listen 80;
	root /var/www/html/site1;
	...
}
server {
	listen 8080;
	root /var/www/site2;
	...
}

# BLOCKING ACCESS
server {
	listen 8080;
	return 444;
	root /var/www/site2;
	...
}
** it is non-standard code for nginx






# Nginx Module
- Nginx is modular
- Nginx modules can be split into five different catagories
- Out-of-Box Modules
Nginx source has some modules that are included by default and they can be enabled or disabled during compile 
time using --with or --without options


# Nginx Architecture
- Master Process is a very effective manager. It manages the resources that, in turn, carry on the actual work 
of serving the client requests
- the worker processes execute their own jobs (waiter)
- cache loader and cache manager deal with cache
** process vs. thread
- Threads reside inside a process, the work is done inside a process using one or many threads.
- each worker process in Nginx is single threaded and runs independently. Their core job is to grab 
new connections and process them as quickly as possible. You can use thread pools in special use cases 
where you can have multiple threads per process (taking care of blocking call or slow customer)
- Just because you have an option of thread pool shouldn't imply that you use it Blocking has a tendency 
to degrade the performance in a BIG way!

# host static websites
- The static sites contain a lot of resources like images, stylesheets, JavaScript files, html, text, PDF, and so on
- The basic nature of the content is that it is made once and served multiple times to the visitors
** Scale Up Vs. Scale Out
	- Scale Up: adding more CPU or RAM might help depending on the workload
	- Scale Out: add more servers in order to keep up with the traffic
- You can host multiple websites on the same server and differentiate them using 
  server blocks (similar to a virtual host in apache)
	## best practices
	1. dont give permission 777 
	2. dont use multiple root directives inside location block
	server {
    server_name www.site1.com;
    location / {
        root /usr/share/nginx/html;
        # [...]
      }
    location /somewhere {
        root /usr/share/nginx/html;
        # [...]
      }
    }
	3. saparate config file for each website
	
# host dynamic websites


# Nginx Loadbalance
1.Round Robin
2.Least Connection, Optionally Weighted
-  the request will be routed to the server with the least number of active connections
3. IP, Generic hash
- The best use case for using hash is probably when you have a dynamic page that makes data intensive operations that are cachable
- Bear in mind that a hash algorithm will most likely not distribute the load evenly

- Nginx load balancer routing to the back-end Nginx servers. This is not a mandatory requirement. You can choose Nginx 
to route traffic to any other web server.

# PASSING THE HOST HEADER
- the request coming from Nginx to the back-end servers will look different than a request coming directly from the client. 
This is because Nginx makes some adjustments to headers that it receives from the client
   location / {
       proxy_set_header HOST $host;
       proxy_pass http://backend;                                                                                          
    }

# FORWARDING IP INFORMATION
- the requests are forwarded to the back end, it has no information about where the requests have actually come from
There are scenarios where you might want to log information about the actual visitors
- use proxy-set-header directive
 location / {
        proxy_set_header HOST $proxy_host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_pass http://backend;
    }
    
# BUFFERING
- turning buffering on will tell Nginx to hold the back-end data temporarily, and feed that data to the client. 
This feature allows the back ends to be freed up quickly since they have to simply work and ensure that the data is fed to client

# NGINX CACHING
- most of static contents make a copy of the data on the load balance or proxy itself, so that the subsequent requests could be 
served directly from the NLB instead of fetching the data every time from the backend servers

proxy_cache_path path levels=1:2 keys_zone=my_cache:10m max_size=10g inactive=60m
